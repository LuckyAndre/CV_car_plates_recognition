{"nbformat":4,"nbformat_minor":5,"metadata":{"notebookId":"a9df0315-39e9-455b-983f-51b82aa411e5","kernelspec":{"name":"python3","display_name":"Yandex DataSphere Kernel","language":"python"},"language_info":{"name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","file_extension":".py","version":"3.7.7","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3}}},"cells":[{"cell_type":"code","source":"%pip install editdistance\n%pip install segmentation-models-pytorch","metadata":{"cellId":"atrr2q7tz9g14mjsc61gx6","trusted":true},"outputs":[{"output_type":"stream","name":"stdout","text":"Defaulting to user installation because normal site-packages is not writeable\nRequirement already satisfied: editdistance in /home/jupyter/.local/lib/python3.7/site-packages (0.5.3)\n\u001B[33mWARNING: You are using pip version 21.0.1; however, version 21.2.4 is available.\nYou should consider upgrading via the '/usr/local/bin/python3 -m pip install --upgrade pip' command.\u001B[0m\nDefaulting to user installation because normal site-packages is not writeable\nRequirement already satisfied: segmentation-models-pytorch in /home/jupyter/.local/lib/python3.7/site-packages (0.2.0)\nRequirement already satisfied: torchvision>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from segmentation-models-pytorch) (0.7.0)\nRequirement already satisfied: efficientnet-pytorch==0.6.3 in /home/jupyter/.local/lib/python3.7/site-packages (from segmentation-models-pytorch) (0.6.3)\nRequirement already satisfied: timm==0.4.12 in /home/jupyter/.local/lib/python3.7/site-packages (from segmentation-models-pytorch) (0.4.12)\nRequirement already satisfied: pretrainedmodels==0.7.4 in /home/jupyter/.local/lib/python3.7/site-packages (from segmentation-models-pytorch) (0.7.4)\nRequirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from efficientnet-pytorch==0.6.3->segmentation-models-pytorch) (1.6.0)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pretrainedmodels==0.7.4->segmentation-models-pytorch) (4.50.0)\nRequirement already satisfied: munch in /home/jupyter/.local/lib/python3.7/site-packages (from pretrainedmodels==0.7.4->segmentation-models-pytorch) (2.5.0)\nRequirement already satisfied: numpy in /kernel/lib/python3.7/site-packages (from torch->efficientnet-pytorch==0.6.3->segmentation-models-pytorch) (1.19.4)\nRequirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch->efficientnet-pytorch==0.6.3->segmentation-models-pytorch) (0.18.2)\nRequirement already satisfied: pillow>=4.1.1 in /kernel/lib/python3.7/site-packages (from torchvision>=0.5.0->segmentation-models-pytorch) (8.3.1)\nRequirement already satisfied: six in /kernel/lib/python3.7/site-packages (from munch->pretrainedmodels==0.7.4->segmentation-models-pytorch) (1.16.0)\n\u001B[33mWARNING: You are using pip version 21.0.1; however, version 21.2.4 is available.\nYou should consider upgrading via the '/usr/local/bin/python3 -m pip install --upgrade pip' command.\u001B[0m\n"}],"execution_count":52},{"cell_type":"code","source":"from argparse import Namespace\n\nfrom scripts.create_submission import main","metadata":{"cellId":"py1mhtnql19zmtipez9zf","trusted":true},"outputs":[],"execution_count":63},{"cell_type":"code","source":"request = Namespace(\n    data_path='data/data',\n    seg_threshold=0.5,\n    seg_model='runs/segmentation_data_10000_image_size_256/CP-best.pth',\n    rec_model='runs/recognition_levenshtein/CP-best.pth',\n    input_wh='320x64',\n    output_file='runs/segmentation_data_10000_image_size_256/segmentation_data_10000_image_size_256_submission.csv'\n)","metadata":{"cellId":"oyrw1nqznpbt1gy5b3bbml","trusted":true},"outputs":[],"execution_count":61},{"cell_type":"code","source":"#!g1.1\nmain(request)","metadata":{"cellId":"diu7pr4ldiph8f3lcope27","trusted":true},"outputs":[{"output_type":"stream","name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/resnet34-333f7ec4.pth\" to /tmp/xdg_cache/torch/hub/checkpoints/resnet34-333f7ec4.pth\n"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=87306240.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"779371b3a8094dc1a1b3ff7a35464b17"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":"Start inference\n\n"},{"output_type":"stream","name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /tmp/xdg_cache/torch/hub/checkpoints/resnet18-5c106cde.pth\n"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=46827520.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9bb8ad80417c473fa0f4a2216cdd14ab"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":"\n"},{"output_type":"stream","name":"stderr","text":"100%|█████████▉| 3154/3157 [02:30<00:00, 17.73it/s]"},{"output_type":"stream","name":"stdout","text":"Done\n"},{"output_type":"stream","name":"stderr","text":"100%|██████████| 3157/3157 [02:31<00:00, 20.90it/s]\n"}],"execution_count":62},{"cell_type":"code","source":"","metadata":{"cellId":"xcom2wlmoej8qp4z1gica"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"request = Namespace(\n    data_path='data/data',\n    seg_threshold=0.5,\n    seg_model='runs/segmentation_data_10000_image_size_512/CP-best.pth',\n    rec_model='runs/recognition_levenshtein/CP-best.pth',\n    input_wh='320x64',\n    output_file='runs/segmentation_data_10000_image_size_512/segmentation_data_10000_image_size_512_submission.csv'\n)","metadata":{"cellId":"fzv0hk17mhvrpvi917d2je","trusted":true},"outputs":[],"execution_count":64},{"cell_type":"code","source":"#!g1.1\nmain(request)","metadata":{"cellId":"l2leic18vihcwfo2w9o8ij","trusted":true},"outputs":[{"output_type":"stream","name":"stdout","text":"Start inference\n"},{"output_type":"stream","name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/resnet34-333f7ec4.pth\" to /tmp/xdg_cache/torch/hub/checkpoints/resnet34-333f7ec4.pth\n"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=87306240.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aecf0ed0203d45d28c19e18f9682a325"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":"\n"},{"output_type":"stream","name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /tmp/xdg_cache/torch/hub/checkpoints/resnet18-5c106cde.pth\n"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=46827520.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b963e3f29ef498690da765d7f985499"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":"  0%|          | 0/3157 [00:00<?, ?it/s]"},{"output_type":"stream","name":"stdout","text":"\n"},{"output_type":"stream","name":"stderr","text":" 70%|██████▉   | 2202/3157 [01:46<00:46, 20.72it/s]\n"},{"output_type":"error","ename":"error","evalue":"OpenCV(4.5.3) /tmp/pip-req-build-l1r0y34w/opencv/modules/imgproc/src/resize.cpp:4051: error: (-215:Assertion failed) !ssize.empty() in function 'resize'\n","traceback":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m","\u001B[0;31merror\u001B[0m                                     Traceback (most recent call last)","\u001B[0;32m<ipython-input-1-8dcdaa05829d>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mmain\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mrequest\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0;31m#\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;32m~/work/resources/contest02/scripts/create_submission.py\u001B[0m in \u001B[0;36mmain\u001B[0;34m(args)\u001B[0m\n\u001B[1;32m     98\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     99\u001B[0m             \u001B[0mcrop\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mimage_src\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0my1\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0my2\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mx1\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mx2\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m:\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;31m# вырезаю ГРЗ из исходного изображения\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 100\u001B[0;31m             \u001B[0mtensor\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mprepare_for_recognition\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcrop\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mw\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mh\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mto\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdevice\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    101\u001B[0m             \u001B[0;32mwith\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mno_grad\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    102\u001B[0m                 \u001B[0mtext\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mrecognition_model\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtensor\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdecode\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;32m~/work/resources/contest02/inference_utils.py\u001B[0m in \u001B[0;36mprepare_for_recognition\u001B[0;34m(image, output_size)\u001B[0m\n\u001B[1;32m     86\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0mprepare_for_recognition\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mimage\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0moutput_size\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     87\u001B[0m     \u001B[0mimage\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mimage\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mastype\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfloat\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m/\u001B[0m \u001B[0;36m255.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 88\u001B[0;31m     \u001B[0mimage\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcv2\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mresize\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mimage\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0moutput_size\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minterpolation\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mcv2\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mINTER_AREA\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     89\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfrom_numpy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mimage\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtranspose\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfloat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0munsqueeze\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;31m# 1 X C x H x W\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;31merror\u001B[0m: OpenCV(4.5.3) /tmp/pip-req-build-l1r0y34w/opencv/modules/imgproc/src/resize.cpp:4051: error: (-215:Assertion failed) !ssize.empty() in function 'resize'\n"]}],"execution_count":65},{"cell_type":"code","source":"","metadata":{"cellId":"gujtt3gykyhfvw9lm0a9l"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"request = Namespace(\n    data_path='data/data',\n    seg_threshold=0.5,\n    seg_model='runs/segmentation_data_10000_image_size_512_bce_05/CP-best.pth',\n    rec_model='runs/recognition_levenshtein/CP-best.pth',\n    input_wh='320x64',\n    output_file='runs/segmentation_data_10000_image_size_512_bce_05/segmentation_data_10000_image_size_512_bce_05_submission.csv'\n)","metadata":{"cellId":"w6tu7bj8a33wrzjmwf1n6","trusted":true},"outputs":[],"execution_count":66},{"cell_type":"code","source":"#!g1.1\nmain(request)","metadata":{"cellId":"spogwufpqyc84mzd333kli","trusted":true},"outputs":[{"output_type":"stream","name":"stdout","text":"Start inference\n"},{"output_type":"stream","name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/resnet34-333f7ec4.pth\" to /tmp/xdg_cache/torch/hub/checkpoints/resnet34-333f7ec4.pth\n"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=87306240.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"23f78bd2befa40ceb62a81f8c9ec14a4"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":"\n"},{"output_type":"stream","name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /tmp/xdg_cache/torch/hub/checkpoints/resnet18-5c106cde.pth\n"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=46827520.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"10c922ac4b5845bb9d6b4b84c42badd4"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":"\n"},{"output_type":"stream","name":"stderr","text":"100%|█████████▉| 3148/3157 [02:31<00:00, 19.76it/s]"},{"output_type":"stream","name":"stdout","text":"Done\n"},{"output_type":"stream","name":"stderr","text":"100%|██████████| 3157/3157 [02:32<00:00, 20.73it/s]\n"}],"execution_count":67},{"cell_type":"code","source":"","metadata":{"cellId":"n72vdlgs977fuux198y9m"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"cellId":"x76djkxqfejpucxjoevs1d"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"cellId":"0t7cac83x33e7aylvy06nec"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"cellId":"bpvtg2l3g99s59bomm2vr"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"cellId":"l99qaxhde7qrjx6k1jad"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"cellId":"39z33d7hh1mfgezcw3absd"},"outputs":[],"execution_count":null}]}